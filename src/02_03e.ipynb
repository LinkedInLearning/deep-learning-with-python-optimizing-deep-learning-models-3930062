{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4762486a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>L1 Regularization of a Deep Learning Model in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "Regularization is a set of techniques designed to reduce overfitting in machine learning models. Overfitting occurs when a model performs exceptionally well on the training data but fails to generalize to unseen data. This often happens when the model becomes overly complex and starts memorizing the noise or irrelevant details in the training data, instead of learning the underlying patterns.\n",
    "\n",
    "Regularization works by introducing constraints or penalties to the training process, limiting the modelâ€™s ability to fit the training data too closely. By controlling the complexity of the model, regularization helps strike a balance between underfitting and overfitting, resulting in better generalization performance.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "+ Know how to apply L1 regularization to a deep learning model.\n",
    "+ Understand how to evaluate the impact of regularization on a deep learning model.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "+ Basic knowledge of Python programming (variables, functions, classes).\n",
    "+ Familiarity with the fundamentals of how to build a deep learning model in Python using Keras.\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, and `matplotlib` packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>1. Import and Preprocess the Data</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "We start by importing the data. For this tutorial, we'll use the **MNIST dataset**, a classic dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. Each image is 28 x 28 pixels, and the dataset is divided into 60,000 training images and 10,000 testing images. Our goal will be to develop a model that learns to correctly identify a handritten digit given the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(1234)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff433d",
   "metadata": {},
   "source": [
    "Our deep learning model expects the images as a vector of size 784 (i.e. 28 $\\times$ 28). So, let's flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28 * 28)\n",
    "test_images = test_images.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7cc6",
   "metadata": {},
   "source": [
    "The model also expects the image pixel values scaled. Let's do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76519",
   "metadata": {},
   "source": [
    "Finally, we also need to one-hot encode the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4194c37",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>2. Define the Baseline Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4fc1998",
   "metadata": {},
   "source": [
    "The baseline model consists of an input layer with 784 nodes, two hidden layers with 512 and 128 nodes (respectively), and an output layer with 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a16da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Input(shape = (784,)),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e344725b",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>3. Compile and Train the Baseline Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5645640a",
   "metadata": {},
   "source": [
    "Next, we compile the baseline model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967a9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6124f2",
   "metadata": {},
   "source": [
    "... train the model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9210495",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs = 15,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39e2d6d",
   "metadata": {},
   "source": [
    "... and plot the training and validation loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca62284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss', marker = 'o')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss', marker = 's')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac16ddd",
   "metadata": {},
   "source": [
    "A clear indicator of overfitting is the divergence in the training and validation loss metrics, which is visible in the training curves above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50553c6e",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b> To learn more about deep learning and how to build a deep learning model in Python using Keras, refer to  the LinkedIn Learning course titled <b>\"Deep Learning with Python: Foundations\"</b>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab5ecd12",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>4. Apply L1 Regularization to the Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0e0850",
   "metadata": {},
   "source": [
    "L1 (Lasso) Regularization adds a penalty proportional to the absolute values of the weights during training. This encourages sparsity, meaning the model learns to rely only on the most important features. Mathematically, the impact of L1 regularization on the loss function is defined as:\n",
    "$$\n",
    "\\text{Loss}_{L1} = \\text{Original Loss} + \\lambda \\cdot \\sum ^n _{i=1} \\lvert w_i \\rvert\n",
    "$$\n",
    "\n",
    "+ **$\\lambda$:** is the regularization paremeter, controlling the strength of the penaly. Higher values of $\\lambda$ lead to stronger regularization.\n",
    "+ **$\\lvert w_i \\rvert$:** are the absolute values of the weights.\n",
    "\n",
    "To apply L1 regularization to the baseline model, we set the `kernel_regularizer` argument within each hidden layer of the network to `l1(0.001)`. This means that the regularization paremeter is set to $0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5375db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l1\n",
    "\n",
    "model_l1 = keras.Sequential([\n",
    "    Input(shape = (784,)),                          \n",
    "    Dense(512, activation = 'relu', kernel_regularizer = l1(0.001)),         \n",
    "    Dense(128, activation = 'relu', kernel_regularizer = l1(0.001)),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92d71dd",
   "metadata": {},
   "source": [
    "Next, we compile the regularized model,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8b12d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l1.compile(\n",
    "    optimizer = 'adam',                                  \n",
    "    loss = 'categorical_crossentropy',                    \n",
    "    metrics = ['accuracy']                                \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0c3e30",
   "metadata": {},
   "source": [
    "... train the regularized model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23cc35a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_l1.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs = 15,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bb4f44",
   "metadata": {},
   "source": [
    "... and plot the training and validation loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d03d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss', marker = 'o')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss', marker = 's')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1972c8b",
   "metadata": {},
   "source": [
    "This time, we see that the two metrics reduce in value at a similar rate as training continues. This indicates that L1 regularization is effectively helping the model generalize better by encouraging sparsity in the learned weights. \n",
    "\n",
    "By penalizing the absolute values of the weights, L1 regularization pushes many weights towards zero, effectively simplifying the model and reducing the risk of overfitting to the training data.\n",
    "\n",
    "The similar rate of reduction for both the training and validation loss metrics suggests that the model is learning patterns that are relevant across both datasets, rather than over-specializing to the training data. The sparsity induced by L1 regularization allows the model to focus only on the most important features, which improves its ability to generalize to unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
