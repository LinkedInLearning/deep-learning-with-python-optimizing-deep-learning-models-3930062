{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4762486a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>Hyperparameter Tuning a Deep Learning Model in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "Hyperparameter tuning is the process of systematically searching for the optimal combination of model parameters prior to training. These parameters, such as the number of hidden layers, the dropout rates between layers, the optimizer learning rate, and training batch size, play a critical role in determining the performance and generalizability of a deep learning model. Effective hyperparameter tuning can significantly improve model accuracy and robustness. In this tutorial, we will use the MNIST digit classification task to illustrate how to leverage Keras Tuner to discover the best configuration of hyperparameters for a deep learning model in Python.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "+ Understand the importance of hyperparameter tuning in deep learning.\n",
    "+ Learn how to define and parameterize a tunable model using Keras Tuner.\n",
    "+ Explore the use of Hyperband, an efficient tuning algorithm, to optimize hyperparameters like layer sizes, dropout rates, learning rates, and batch sizes.\n",
    "+ Evaluate the performance of a tuned model on unseen data and compare it to a baseline model.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "+ Basic knowledge of Python programming (variables, functions, classes).\n",
    "+ Familiarity with the fundamentals of how to build a deep learning model in Python using Keras.\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, and `keras_tuner`packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a7ab1e-8abb-4063-836f-153b0a2d7e97",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b>To learn more about deep learning and how to build a deep learning model in Python using Keras, refer to  the LinkedIn Learning course titled <b>\"Deep Learning with Python: Foundations\"</b>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>1. Import and Preprocess the Data</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "We start by importing the data. For this tutorial, we'll use the **MNIST dataset**, a classic dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. Each image is 28 x 28 pixels, and the dataset is divided into 60,000 training images and 10,000 testing images. Our goal will be to develop a model that learns to correctly identify a handritten digit given the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(1234)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff433d",
   "metadata": {},
   "source": [
    "Our deep learning model expects the images as a vector of size 784 (i.e. 28 $\\times$ 28). So, let's flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28 * 28)\n",
    "test_images = test_images.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7cc6",
   "metadata": {},
   "source": [
    "The model also expects the image pixel values scaled. Let's do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76519",
   "metadata": {},
   "source": [
    "Finally, we also need to one-hot encode the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c41f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>2. Define the Baseline Model Architecture</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c61be",
   "metadata": {},
   "source": [
    "The baseline model consists of an input layer with 784 nodes, two hidden layers with 32 and 16 nodes (respectively), and an output layer with 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Input(shape = (784,)),\n",
    "    Dense(32, activation = 'relu'),\n",
    "    Dense(16, activation = 'relu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a222d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>3. Train and Evaluate the Baseline Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906d178",
   "metadata": {},
   "source": [
    "Next, we compile and train the baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a769eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs = 10,\n",
    "    validation_split = 0.1,\n",
    "    batch_size = 128\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6312f3d",
   "metadata": {},
   "source": [
    "Finally, we evaluate the model's performance against the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e7da3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>4. Define the Tunable Model Architecture</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de094de4",
   "metadata": {},
   "source": [
    "Before we search for the optimal hyperparameters for our model, we need to define a function that specifies the architectural blueprint of the model. The blueprint will incorporate hyperparameters for the number of units per layer, dropout rates, and the optimizer learning rate. Keras Tuner will invoke this function multiple times with different hyperparameter values in order to find an optimal combination that maximizes validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(Input(shape = (784,)))\n",
    "    \n",
    "    # Tune number of units for the first hidden layer\n",
    "    model.add(Dense(units = hp.Int('hidden1', 32, 512, step = 32), activation = 'relu'))\n",
    "\n",
    "    # Tune dropout rate after the first hidden layer\n",
    "    model.add(Dropout(rate = hp.Float('dropout1', 0.1, 0.5, step = 0.1)))\n",
    "    \n",
    "    # Tune number of units for the second hidden layer\n",
    "    model.add(Dense(units = hp.Int('hidden2', 16, 128, step = 16), activation = 'relu'))\n",
    "    \n",
    "    # Tune dropout rate after the second hidden layer\n",
    "    model.add(Dropout(rate = hp.Float('dropout2', 0.1, 0.5, step = 0.1)))\n",
    "\n",
    "    # Output layer with 10 units for the 10 classes\n",
    "    model.add(Dense(units = 10, activation = 'softmax'))\n",
    "    \n",
    "    # Tune the learning rate for the optimizer\n",
    "    # Choose an optimal value from 0.0001, 0.001, or 0.01\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [1e-4, 1e-3, 1e-2])\n",
    "\n",
    "    \n",
    "    # Compile the tunable model\n",
    "    model.compile(\n",
    "        optimizer = Adam(learning_rate = hp_learning_rate),\n",
    "        loss = 'categorical_crossentropy',\n",
    "        metrics = ['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59fc654-634c-4c83-a94e-fddc608e9e9a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>5. Running the Hyperparameter Search</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be366bd",
   "metadata": {},
   "source": [
    "Having defined our hyperparameter-tunable model, we now need to set up the tuner. Here, we choose **Hyperband**, a resource-efficient approach to hyperparameter tuning that builds upon random search and combines it with the principles of early stopping. Its primary goal is to reduce the computational cost of hyperparameter optimization by dynamically allocating more resources to promising hyperparameter configurations and fewer resources to less promising ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e4764",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "76301446",
   "metadata": {},
   "source": [
    "We will now start the search process with `tuner.search()`. This command will build and train multiple models using different combinations of hyperparameters. It will try different layer sizes, dropout rates, learning rates, and batch sizes, evaluating each combination’s performance on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f4f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "227dccce-65b3-4f8d-b856-f2480c69e4b1",
   "metadata": {},
   "source": [
    "Once the search is complete, we can output the best configuration of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9b94383-bb59-40fd-ab67-c175621ba385",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(\n",
    "    f\"\\nThe optimal number of units in the first and second densely-connected \"\n",
    "    f\"layers are {best_hps.get('hidden1')} with a dropout rate of {best_hps.get('dropout1'):.2f} and \"\n",
    "    f\"{best_hps.get('hidden2')} with a dropout rate of {best_hps.get('dropout2'):.2f}, respectively. \"\n",
    "    f\"The optimal learning rate for the optimizer is {best_hps.get('learning_rate')} and \"\n",
    "    f\"the optimal training batch size is {best_hps.get('batch_size')}.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b271799d-874e-4e51-93aa-28319d87bf6e",
   "metadata": {},
   "source": [
    "Next, using the optimal set of hyperparameters, we create a tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e300269b-19e3-465b-af80-1f18752515a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = tuner.hypermodel.build(best_hps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef54d25-440a-4ad6-babf-1d576445da34",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>6. Train and Evaluate the Tuned Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ed7fd",
   "metadata": {},
   "source": [
    "Finally, we train the tuned model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24af3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ab3d40f-63b6-4e28-882d-4c5566c1794f",
   "metadata": {},
   "source": [
    "... and eveluate how well the tuned model generalizes to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ba68d7-efcb-4281-a689-a01a09ce991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = best_model.evaluate(test_images, test_labels)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd4fe0",
   "metadata": {},
   "source": [
    "This time, we see that the performance of the tuned model on the test data outperforms that of the baseline model. By systematically searching through a range of potential hyperparameters, the tuner identifies a configuration that better balances the complexity of the network with the demands of the data. This improved balance helps the model capture relevant patterns without overfitting, leading to higher accuracy on unseen examples. \n",
    "\n",
    "For many problems, careful hyperparameter tuning can yield substantial gains over default or intuitively chosen parameters. It also provides insights into which aspects of the model’s architecture and training process are most influential for performance, guiding future experimentation and development."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
