{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4762486a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>Building a More Robust Deep Learning Model in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "Deep learning models can often be made more stable, efficient, and generalizable by introducing techniques that address common training pitfalls. Methods like **Batch Normalization** mitigate internal covariate shift, **Gradient Clipping** keeps parameters from exploding, **Early Stopping** helps prevent overfitting, and **Learning Rate Scheduling** fine-tunes the optimization process over time. In this tutorial, we will illustrate how to apply each of these techniques in the context of the MNIST digit classification task using Keras.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "+ Understand how to use Batch Normalization to stabilize and accelerate training.\n",
    "+ Learn how to use Gradient Clipping to control exploding gradients.\n",
    "+ Apply Early Stopping to preserve your best-trained model and prevent overfitting.\n",
    "+ Explore Learning Rate Scheduling to dynamically adjust the learning rate based on validation performance.\n",
    "+ Combine all these techniques to build a more robust and accurate deep learning model compared to a baseline approach.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "+ Basic knowledge of Python programming (variables, functions, classes).\n",
    "+ Familiarity with the fundamentals of how to build a deep learning model in Python using Keras.\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, and `matplotlib` packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd56d289-a6bb-4954-97a9-0c75108e7b83",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b>To learn more about deep learning and how to build a deep learning model using Keras in Python, refer to  the LinkedIn Learning course titled <b>\"Deep Learning with Python: Foundations\"</b>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>1. Import and Preprocess the Data</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "We start by importing the data. For this tutorial, we'll use the **MNIST dataset**, a classic dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. Each image is 28 x 28 pixels, and the dataset is divided into 60,000 training images and 10,000 testing images. Our goal will be to develop a model that learns to correctly identify a handritten digit given the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(1234)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff433d",
   "metadata": {},
   "source": [
    "Our deep learning model expects the images as a vector of size 784 (i.e. 28 $\\times$ 28). So, let's flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28 * 28)\n",
    "test_images = test_images.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7cc6",
   "metadata": {},
   "source": [
    "The model also expects the image pixel values scaled. Let's do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76519",
   "metadata": {},
   "source": [
    "Finally, we also need to one-hot encode the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c41f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>2. Define the Model with Batch Normalization</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c61be",
   "metadata": {},
   "source": [
    "Our model consists of an input layer with 784 nodes, two hidden layers with 512 and 128 nodes (respectively), and an output layer with 10 nodes. Between each of the hidden layers, we will normalize the outputs of one layer before feeding them into the next. This is known as **Batch Normalization**. Batch Normalization can stabilize the training of a deep learning model and help it converge faster. \n",
    "\n",
    "To apply BatchNormalization to a model, we simply include a `BatchNormalization()` layer to the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, BatchNormalization\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Input(shape = (784,)),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a222d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>3. Compile the Model with Gradient Clipping</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906d178",
   "metadata": {},
   "source": [
    "Now that we've defined our model's architecture, let's compile it by specifying the optimizer, loss function and performance metric to optimize. We choose to use the `Adam()` optimizer for our model. By default, optimizers don’t impose any bounds on gradients. However, large gradients can cause a model’s parameters to fluctuate significantly during training and hamper convergence. **Gradient Clipping** mitigates this issue by limiting the magnitude (or norm) of gradients.\n",
    "\n",
    "To implement Gradient Clipping, we set the `clipnorm` argument within the `Adam()` optimizer to `1.0`. This ensures that the L2 norm of the gradients do not exceed 1.0 during training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a769eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "model.compile(\n",
    "    optimizer = Adam(clipnorm = 1.0),\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12f9b792-f780-43aa-a7cc-555099332d8b",
   "metadata": {},
   "source": [
    "Note that we can adjust the `clipnorm` value as we see fit based on our dataset or problem. Alternatively, we could have also used the `clipvalue` argument to clip gradiens by value instead of by norm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e473b46-8814-46b5-b14f-c50834084ef4",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>4. Train the Model using Callbacks</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f50199b-5e6d-409d-b954-c8c961b3e638",
   "metadata": {},
   "source": [
    "In Keras, a **callback** is an object that can perform custom actions at specific points during the training process, such as at the end of each epoch or batch. One of the most commonly used callbacks is **EarlyStopping**, which monitors a particular metric (e.g., validation loss) and stops training if that metric does not improve after a specified number of epochs (known as patience). This helps prevent overfitting by halting training when the model has reached its optimal performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d8704a-2518-4475-baa4-38513d92e4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor = 'val_loss', \n",
    "    patience = 3, \n",
    "    restore_best_weights = True\n",
    ")   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee29891-ae45-492a-a29b-4316d619544e",
   "metadata": {},
   "source": [
    "+ `monitor = 'val_loss'`: This tells EarlyStopping to track the validation loss. You can change this to `'val_accuracy'` or another relevant metric if preferred.\n",
    "+ `patience = 3`: If the monitored metric does not improve for 3 consecutive epochs, training will stop. This patience value can be tuned based on how quickly your metric tends to plateau.\n",
    "+ `restore_best_weights = True`: Once EarlyStopping concludes training, the model automatically reverts to the weights from the best-performing epoch (lowest validation loss). This ensures you retain the optimal model parameters even if subsequent epochs lead to worse performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ebcdb8-d575-4d4f-affb-ba85b4d20384",
   "metadata": {},
   "source": [
    "Another commonly used callback in Keras is `ReduceLROnPlateau`, which implements a **Learning Rate Scheduling** strategy. With this callback, the learning rate is automatically reduced when a monitored metric (e.g., validation loss) has stopped improving. Reducing the learning rate during training can help the model escape local minima or plateaus and often leads to better convergence. This is especially useful toward the later stages of training when larger steps (learning rates) might cause oscillations or prevent fine-tuning the model parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df6400-0924-41c8-8f50-b44a69a13742",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "lrate_schedule = ReduceLROnPlateau(\n",
    "    monitor = 'val_loss',\n",
    "    factor = 0.1, \n",
    "    patience = 2,\n",
    "    min_lr = 1e-6\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5386d5cd-4c0e-40c9-a24f-ad06dbca929d",
   "metadata": {},
   "source": [
    "+ `monitor = 'val_loss'`: Similar to EarlyStopping, this callback watches the validation loss for improvements. You can also monitor other metrics such as `'val_accuracy'`.\n",
    "+ `factor = 0.1`: Every time the monitored metric fails to improve after a specified patience period, the current learning rate is multiplied by `0.5`. For instance, if your learning rate was $0.001$, it will be reduced to $0.0001$.\n",
    "+ `patience = 2`: This tells the callback to wait for 2 consecutive epochs of no improvement in validation loss before reducing the learning rate.\n",
    "+ `min_lr = 1e-6`: Ensures that the learning rate never goes below $10^{-6}$. This prevents the learning rate from becoming so small that training effectively stalls."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e098f3-ca78-4492-b1ee-8f899aa7ba2a",
   "metadata": {},
   "source": [
    "After defining our callbacks, we can combine them into a list and pass them to the callbacks argument in the `fit()` method. This way, both callbacks will be activated during each training epoch, helping us avoid overfitting and fine-tune the model’s learning rate for improved convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b6e01c-c31c-415d-8798-a00f7c4cfca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_callbacks = [early_stopping, lrate_schedule]\n",
    "\n",
    "history = model.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs = 20,\n",
    "    validation_split = 0.1,\n",
    "    batch_size = 128,\n",
    "    callbacks = my_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc88bf6-8930-45d5-9fbf-dba21f5628b9",
   "metadata": {},
   "source": [
    "Notice that even though we specified 20 epochs within the `fit()` method, the training process stopped early. Early Stopping detected that the validation loss had not improved for the configured patience period of 3, and so it halted training before reaching epoch 20.\n",
    "\n",
    "Let's plot the training and validation loss metrics to get a better sense of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fec6c3-e8ae-44a4-80d6-8d6c889fedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss', marker = 'o')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss', marker = 's')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd4fe0",
   "metadata": {},
   "source": [
    "Having added batch normalization, gradient clipping, early stopping, and learning rate scheduling to our training process, we have introduced multiple methods that help stabilize and optimize our deep learning model’s behavior. Each of these techniques address different potential pitfalls in model training, from exploding gradients to overfitting. Together, they yield a model that converges more reliably and generalizes better on unseen data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
