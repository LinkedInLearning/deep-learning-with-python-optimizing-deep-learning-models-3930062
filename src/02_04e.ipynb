{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4762486a",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h1>L2 Regularization of a Deep Learning Model in Python</h1></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11963cc7",
   "metadata": {},
   "source": [
    "Regularization is a set of techniques designed to reduce overfitting in machine learning models. Overfitting occurs when a model performs exceptionally well on the training data but fails to generalize to unseen data. This often happens when the model becomes overly complex and starts memorizing the noise or irrelevant details in the training data, instead of learning the underlying patterns.\n",
    "\n",
    "Regularization works by introducing constraints or penalties to the training process, limiting the modelâ€™s ability to fit the training data too closely. By controlling the complexity of the model, regularization helps strike a balance between underfitting and overfitting, resulting in better generalization performance.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this tutorial, you will:\n",
    "+ Know how to apply L2 regularization to a deep learning model.\n",
    "+ Understand how to evaluate the impact of regularization on a deep learning model.\n",
    "\n",
    "\n",
    "## Prerequisites\n",
    "Before we begin, ensure you have:\n",
    "+ Basic knowledge of Python programming (variables, functions, classes).\n",
    "+ Familiarity with the fundamentals of how to build a deep learning model in Python using Keras.\n",
    "+ A Python (version 3.x) environment with the `tensorflow`, `keras`, and `matplotlib` packages installed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b0ce67-2f1f-48cf-b1f8-e1951a62fa4c",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><b>Note:</b>To learn more about deep learning and how to build a deep learning model in Python using Keras, refer to  the LinkedIn Learning course titled <b>\"Deep Learning with Python: Foundations\"</b>.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2887bb87",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>1. Import and Preprocess the Data</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c96c17b",
   "metadata": {},
   "source": [
    "We start by importing the data. For this tutorial, we'll use the **MNIST dataset**, a classic dataset in the machine learning community. It consists of 70,000 grayscale images of handwritten digits ranging from 0 to 9. Each image is 28 x 28 pixels, and the dataset is divided into 60,000 training images and 10,000 testing images. Our goal will be to develop a model that learns to correctly identify a handritten digit given the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c664ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "keras.utils.set_random_seed(1234)\n",
    "(train_images, train_labels), (test_images, test_labels) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffff433d",
   "metadata": {},
   "source": [
    "Our deep learning model expects the images as a vector of size 784 (i.e. 28 $\\times$ 28). So, let's flatten the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fee70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.reshape(60000, 28 * 28)\n",
    "test_images = test_images.reshape(10000, 28 * 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fb7cc6",
   "metadata": {},
   "source": [
    "The model also expects the image pixel values scaled. Let's do that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d2d184",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images.astype('float32') / 255\n",
    "test_images = test_images.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cf76519",
   "metadata": {},
   "source": [
    "Finally, we also need to one-hot encode the image labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9454177f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "train_labels = keras.utils.to_categorical(train_labels, num_classes)\n",
    "test_labels = keras.utils.to_categorical(test_labels, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c41f2",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>2. Define the Baseline Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "980c61be",
   "metadata": {},
   "source": [
    "The baseline model consists of an input layer with 784 nodes, two hidden layers with 512 and 128 nodes (respectively), and an output layer with 10 nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92e0d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "model = keras.Sequential([\n",
    "    Input(shape = (784,)),\n",
    "    Dense(512, activation = 'relu'),\n",
    "    Dense(128, activation = 'relu'),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "911a222d",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>3. Compile and Train the Baseline Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e906d178",
   "metadata": {},
   "source": [
    "Next, we compile the baseline model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a769eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6312f3d",
   "metadata": {},
   "source": [
    "... train the model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0df6983",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs = 15,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace0d00b",
   "metadata": {},
   "source": [
    "... and plot the training and validation loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0545b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss', marker = 'o')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss', marker = 's')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c4c7ea",
   "metadata": {},
   "source": [
    "A clear indicator of overfitting is the divergence in the training and validation loss metrics, which is visible in the training curves above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940e7da3",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\"><h2>4. Apply L2 Regularization to the Model</h2></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de094de4",
   "metadata": {},
   "source": [
    "L2 (Ridge) Regularization adds a penalty proportional to the squared values of the weights. This discourages large weights, helping the model generalize better. Mathematically, the impact of L2 regularization on the loss function is defined as:\n",
    "$$\n",
    "\\text{Loss} = \\text{Original Loss} + \\lambda \\cdot \\sum ^n _{i=1} w_i^2\n",
    "$$\n",
    "\n",
    "+ **$\\lambda$:** is the regularization paremeter, controlling the strength of the penaly. Higher values of $\\lambda$ lead to stronger regularization.\n",
    "+ **$w_i^2$:** are the squared values of the weights.\n",
    "\n",
    "To apply L2 regularization to the baseline model, we set the `kernel_regularizer` argument within each hidden layer of the network to `l2(0.001)`. This means that the regularization paremeter is set to $0.001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caba5d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "model_l2 = keras.Sequential([\n",
    "    Input(shape = (784,)),                          \n",
    "    Dense(512, activation = 'relu', kernel_regularizer = l2(0.001)),         \n",
    "    Dense(128, activation = 'relu', kernel_regularizer = l2(0.001)),\n",
    "    Dense(10, activation = 'softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2be366bd",
   "metadata": {},
   "source": [
    "Next, we compile the regularized model,..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2e4764",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l2.compile(\n",
    "    optimizer = 'adam',                                  \n",
    "    loss = 'categorical_crossentropy',                    \n",
    "    metrics = ['accuracy']                                \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76301446",
   "metadata": {},
   "source": [
    "... train the regularized model, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6f4f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_l2.fit(\n",
    "    train_images, \n",
    "    train_labels,\n",
    "    epochs = 15,\n",
    "    batch_size = 128,\n",
    "    validation_split = 0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963ed7fd",
   "metadata": {},
   "source": [
    "... and plot the training and validation loss metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d24af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (8, 6))\n",
    "plt.plot(history.history['loss'], label = 'Training Loss', marker = 'o')\n",
    "plt.plot(history.history['val_loss'], label = 'Validation Loss', marker = 's')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cd4fe0",
   "metadata": {},
   "source": [
    "This time, we see that the model's training and validation loss metrics tend to decrease at a similar rate throughout the training process. This is a strong indication that L2 regularization is effectively reducing overfitting by discouraging the model from relying too heavily on large weight values. \n",
    "\n",
    "By adding a penalty proportional to the squared magnitude of the weights, L2 regularization ensures that the model focuses on learning patterns that generalize well to unseen data rather than memorizing the training data.\n",
    "\n",
    "The similarity in the rate of decrease for both metrics indicates that the model is improving its performance on both the training and validation datasets without overfitting. This balanced behavior suggests that the L2 penalty is successfully constraining the complexity of the model, allowing it to capture meaningful relationships in the data while avoiding over-reliance on specific features or noise."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
